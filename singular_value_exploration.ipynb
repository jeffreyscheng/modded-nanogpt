{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "class GeneralizedNewtonSchulz(nn.Module):\n",
    "    def __init__(self, degree: int = 3, num_iterations: int = 5, init_coefficients: Optional[List[float]] = None):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.num_iterations = num_iterations\n",
    "        if init_coefficients is None:\n",
    "            self.coefficients = [nn.Parameter(torch.randn(self.num_polynomial_terms)) for _ in range(self.num_iterations)]\n",
    "        else:\n",
    "            self.verify_init_coefficients_shape(init_coefficients)\n",
    "            self.coefficients = nn.ParameterList([\n",
    "                nn.Parameter(torch.tensor(layer_coeff) if init_coefficients is not None \n",
    "                            else torch.randn(self.num_polynomial_terms))\n",
    "                for layer_coeff in (init_coefficients or [None] * num_iterations)\n",
    "            ])\n",
    "        self.initial_scale = 1.1\n",
    "    \n",
    "    @property\n",
    "    def num_polynomial_terms(self) -> int:\n",
    "        return (self.degree + 1) // 2\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = X / torch.norm(X, p='fro', dim=(-2, -1), keepdim=True) * 1.1\n",
    "\n",
    "        for layer_idx in range(self.num_iterations):\n",
    "            XT = X.transpose(-2, -1)\n",
    "            XTX = torch.bmm(XT, X)\n",
    "            terms = [X]\n",
    "            \n",
    "            for _ in range(self.num_polynomial_terms - 1):\n",
    "                terms.append(torch.bmm(terms[-1], XTX))\n",
    "            \n",
    "            X = sum(coeff * term for coeff, term in zip(self.coefficients[layer_idx], terms))\n",
    "            \n",
    "        return X\n",
    "\n",
    "    def print_polynomial_at_layer(self, layer_idx: int) -> str:\n",
    "        terms = [f\"{coeff:.3f}X(X^TX)^{i}\" if i else f\"{coeff:.3f}X\" \n",
    "                for i, coeff in enumerate(self.coefficients[layer_idx].detach().cpu().tolist())\n",
    "                if abs(coeff) > 1e-6]\n",
    "        return \" + \".join(terms)\n",
    "\n",
    "    def evaluate_scalar_at_layer(self, x: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
    "        powers = torch.tensor([2 * i + 1 for i in range(len(self.coefficients))])\n",
    "        return sum(coeff * torch.pow(x, power) for coeff, power in zip(self.coefficients[layer_idx], powers))\n",
    "    \n",
    "    def evaluate_scalar_across_layers(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer_idx in range(self.num_iterations):\n",
    "            x = self.evaluate_scalar_at_layer(x, layer_idx)\n",
    "        return x\n",
    "\n",
    "    def verify_init_coefficients_shape(self, initial_coefficients: List[float]):\n",
    "        assert len(initial_coefficients) == self.num_iterations\n",
    "        for layer_coeff in initial_coefficients:\n",
    "            assert len(layer_coeff) == self.num_polynomial_terms\n",
    "\n",
    "class SingularValuesDataset(Dataset):\n",
    "    def __init__(self, checkpoint_dir: str, layer_filter: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Dataset for loading singular values from JSON files.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_dir: Directory containing JSON files with singular values\n",
    "            layer_filter: Optional list of layer names to include. If None, all layers are used.\n",
    "        \"\"\"\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.layer_filter = layer_filter\n",
    "        \n",
    "        # Get all JSON files in the directory\n",
    "        file_paths = sorted(\n",
    "            [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith('.json')],\n",
    "            key=lambda x: int(os.path.basename(x).split('.')[0])  # Sort by minibatch index\n",
    "        )\n",
    "        \n",
    "        if not file_paths:\n",
    "            raise ValueError(f\"No JSON files found in {checkpoint_dir}\")\n",
    "        \n",
    "        # Load all singular values into memory\n",
    "        self.all_singular_values = []\n",
    "        \n",
    "        # Load the first file to get layer names if no filter is provided\n",
    "        with open(file_paths[0], 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if self.layer_filter is None:\n",
    "                self.layer_filter = list(data.keys())\n",
    "        \n",
    "        # Load all files and extract singular values\n",
    "        print(f\"Loading singular values from {len(file_paths)} files...\")\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Flatten all singular values across layers into a single pool\n",
    "            for values in data.values():\n",
    "                if len(values) > 0:  # Only add non-empty lists\n",
    "                    self.all_singular_values.extend(values)\n",
    "        \n",
    "        # Convert to tensor for efficiency\n",
    "        self.all_singular_values = torch.tensor(self.all_singular_values, dtype=torch.float32)\n",
    "        print(f\"Loaded {len(self.all_singular_values)} singular values\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.all_singular_values)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get a single singular value as a matrix for the model.\n",
    "        We construct a diagonal matrix with the singular value.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (dim, dim) representing a diagonal matrix\n",
    "        \"\"\"\n",
    "        # Create a small diagonal matrix with the singular value\n",
    "        dim = 10  # Arbitrary dimension, adjust as needed\n",
    "        sv = self.all_singular_values[idx]\n",
    "        \n",
    "        # Create diagonal matrix with the singular value\n",
    "        matrix = torch.zeros(dim, dim)\n",
    "        matrix[0, 0] = sv\n",
    "        \n",
    "        # Add some small random noise to other elements to avoid exact zeros\n",
    "        # This makes the problem more realistic while preserving the dominant singular value\n",
    "        noise = torch.randn(dim, dim) * 1e-6\n",
    "        matrix = matrix + noise\n",
    "        \n",
    "        return matrix\n",
    "\n",
    "def create_dataloader(checkpoint_dirs: List[str], batch_size: int = 8) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader for singular values from one or more checkpoint directories.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dirs: List of directories containing JSON files with singular values\n",
    "        batch_size: Batch size for the DataLoader\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: DataLoader for the SingularValuesDataset\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    \n",
    "    for checkpoint_dir in checkpoint_dirs:\n",
    "        datasets.append(SingularValuesDataset(checkpoint_dir))\n",
    "    \n",
    "    # Combine all datasets\n",
    "    if len(datasets) == 1:\n",
    "        combined_dataset = datasets[0]\n",
    "    else:\n",
    "        combined_dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "    \n",
    "    return DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "def iterate(fn, num_iterations: int = 10):\n",
    "    def wrapper(x):\n",
    "        result = x\n",
    "        for _ in range(num_iterations):\n",
    "            result = fn(result)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def norm_of_xtx_minus_i(X):\n",
    "    I = torch.eye(X.size(-1)).to(X.device)\n",
    "    return torch.norm(X.transpose(-2, -1) @ X - I, p='fro')\n",
    "\n",
    "def derivative_at_zero(model):\n",
    "    # product of zeroth element of each layer's coefficients\n",
    "    return torch.prod(torch.stack([coeff[0] for coeff in model.coefficients]), dim=0).pow(2.0/model.num_iterations)\n",
    "\n",
    "\n",
    "def train_newton_schulz(config: Dict[str, Any]):\n",
    "    device = torch.device(config['device'])\n",
    "    \n",
    "    model = GeneralizedNewtonSchulz(\n",
    "        degree=config['degree'],\n",
    "        num_iterations=config['num_iterations'],\n",
    "        init_coefficients=config['init_coefficients']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        betas=config['adam_betas']\n",
    "    )\n",
    "    \n",
    "    dataloader = create_dataloader(config['checkpoint_dirs'], config['batch_size'])\n",
    "    \n",
    "    def plot_fn(fn, title):\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        x = torch.linspace(-0.5, 2.0, 10000).to(device)\n",
    "        y = fn(x).cpu().detach()\n",
    "        ax.plot(x.cpu().numpy(), y.numpy())\n",
    "        \n",
    "        # Handle multi-line title\n",
    "        ax.set_title(title, y=1.05, pad=10)\n",
    "        ax.set_ylim(-1, 3)\n",
    "        ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Adjust layout to prevent title cutoff\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        total_orthogonality_loss = 0\n",
    "        total_derivative_loss = 0\n",
    "\n",
    "        for i, matrices in enumerate(dataloader):\n",
    "            matrices = matrices.to(device)\n",
    "            output = model(matrices)\n",
    "            orthogonality_loss = norm_of_xtx_minus_i(output)\n",
    "            derivative_loss = derivative_at_zero(model)\n",
    "            loss = orthogonality_loss - config['alpha'] * derivative_loss\n",
    "            \n",
    "            # Backprop and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            total_orthogonality_loss += orthogonality_loss.item()\n",
    "            total_derivative_loss += derivative_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Batch {i}: Ortho loss {orthogonality_loss.item():.6f}, Deriv loss {derivative_loss.item():.6f}, loss {loss.item():.6f}\")\n",
    "        \n",
    "        # End of epoch reporting\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_ortho_loss = total_orthogonality_loss / num_batches\n",
    "        avg_deriv_loss = total_derivative_loss / num_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Avg loss {avg_loss:.6f}, Avg ortho loss {avg_ortho_loss:.6f}, Avg deriv loss {avg_deriv_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading singular values from 1646 files...\n",
      "Loaded 87646208 singular values\n",
      "Batch 0: Ortho loss 33.862671, Deriv loss 10.667524, loss 33.862671\n",
      "Batch 10: Ortho loss 33.742851, Deriv loss 11.037832, loss 33.742851\n",
      "Batch 20: Ortho loss 33.920349, Deriv loss 11.169354, loss 33.920349\n",
      "Batch 30: Ortho loss 33.928726, Deriv loss 11.439039, loss 33.928726\n",
      "Batch 40: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 50: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 60: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 70: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 80: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 90: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 100: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 110: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 120: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 130: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 140: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 150: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 160: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 170: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 180: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 190: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 200: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 210: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 220: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 230: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 240: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 250: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 260: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 270: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 280: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 290: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 300: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 310: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 320: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 330: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 340: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 350: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 360: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 370: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 380: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 390: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 400: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 410: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 420: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 430: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 440: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 450: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 460: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 470: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 480: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 490: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 500: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 510: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 520: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 530: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 540: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 550: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 560: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 570: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 580: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 590: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 600: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 610: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 620: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 630: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 640: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 650: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 660: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 670: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 680: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 690: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 700: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 710: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 720: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 730: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 740: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 750: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 760: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 770: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 780: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 790: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 800: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 810: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 820: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 830: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 840: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 850: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 860: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 870: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 880: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 890: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 900: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 910: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 920: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 930: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 940: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 950: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 960: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 970: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 980: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 990: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1000: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1010: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1020: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1030: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1040: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1050: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1060: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1070: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1080: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1090: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1100: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1110: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1120: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1130: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1140: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1150: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1160: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1170: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1180: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1190: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1200: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1210: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1220: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1230: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1240: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1250: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1260: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1270: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1280: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1290: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1300: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1310: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1320: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1330: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1340: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1350: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1360: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1370: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1380: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1390: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1400: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1410: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1420: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1430: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1440: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1450: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1460: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1470: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1480: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1490: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1500: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1510: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1520: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1530: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1540: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1550: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1560: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1570: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1580: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1590: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1600: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1610: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1620: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1630: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1640: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1650: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1660: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1670: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1680: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1690: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1700: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1710: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1720: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1730: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1740: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1750: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1760: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1770: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1780: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1790: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1800: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1810: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1820: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1830: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1840: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1850: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1860: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1870: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1880: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1890: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1900: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1910: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1920: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1930: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1940: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1950: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1960: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1970: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1980: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 1990: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2000: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2010: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2020: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2030: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2040: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2050: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2060: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2070: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2080: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2090: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2100: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2110: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2120: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2130: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2140: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2150: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2160: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2170: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2180: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2190: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2200: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2210: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2220: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2230: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2240: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2250: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2260: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2270: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2280: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2290: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2300: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2310: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2320: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2330: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2340: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2350: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2360: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2370: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2380: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2390: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2400: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2410: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2420: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2430: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2440: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2450: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2460: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2470: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2480: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2490: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2500: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2510: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2520: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2530: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2540: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2550: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2560: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2570: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2580: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2590: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2600: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2610: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2620: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2630: Ortho loss nan, Deriv loss nan, loss nan\n",
      "Batch 2640: Ortho loss nan, Deriv loss nan, loss nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m default_config = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcheckpoint_dirs\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33msingular_values/42155f96-2e99-4f36-8745-dd903b02b18f\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     ]\n\u001b[32m     20\u001b[39m }\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run single training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtrain_newton_schulz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mtrain_newton_schulz\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Backprop and optimize\u001b[39;00m\n\u001b[32m    234\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m optimizer.step()\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# Track metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/modded-nanogpt/venv-nanogpt/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/modded-nanogpt/venv-nanogpt/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/modded-nanogpt/venv-nanogpt/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "default_config = {\n",
    "    \"device\": \"cuda\",\n",
    "    \"checkpoint_dirs\": [\"singular_values/42155f96-2e99-4f36-8745-dd903b02b18f\"],\n",
    "    \"degree\": 5,\n",
    "    \"num_iterations\": 7,\n",
    "    \"learning_rate\": 1e-2,\n",
    "    \"adam_betas\": (0.9, 0.9),\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 200,\n",
    "    \"alpha\": 0,\n",
    "    \"init_coefficients\": [\n",
    "        (4.0848, -6.8946, 2.9270),\n",
    "        (3.9505, -6.3029, 2.6377),\n",
    "        (3.7418, -5.5913, 2.3037),\n",
    "        (2.8769, -3.1427, 1.2046),\n",
    "        (2.8366, -3.0525, 1.2012),\n",
    "        (2.8366, -3.0525, 1.2012),\n",
    "        (2.8366, -3.0525, 1.2012),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run single training\n",
    "train_newton_schulz(default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
